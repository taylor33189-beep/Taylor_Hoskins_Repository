{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOi4zN+vrJbTHDr5IW3xa9D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taylor33189-beep/Taylor_Hoskins_Repository/blob/main/Taylor_H_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-priki9wRbW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57b59cd4",
        "outputId": "587e41fc-00c2-4614-983c-418853287321"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.formula.api as smf\n",
        "import itertools\n",
        "\n",
        "# Data loading and preparation (assuming df, train_df, test_df,\n",
        "# independent_variables exist)\n",
        "# Example placeholders if not defined:\n",
        "# df = pd.read_csv('/content/EX9_12-1.DAT', delim_whitespace=True)\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "# independent_variables = ['age', 'alcohol', 'chol', 'fiber']\n",
        "\n",
        "# Generate all predictor combinations\n",
        "all_predictor_combinations = []\n",
        "for i in range(1, len(independent_variables) + 1):\n",
        "    for subset in itertools.combinations(independent_variables, i):\n",
        "        all_predictor_combinations.append(list(subset))\n",
        "\n",
        "\n",
        "# --- Dynamically Implement All-Possible Regressions ---\n",
        "all_possible_aic_scores = {}\n",
        "\n",
        "for predictors in all_predictor_combinations:\n",
        "    formula = 'hdl ~ ' + ' + '.join(predictors)\n",
        "    model = smf.ols(formula=formula, data=train_df).fit()\n",
        "    all_possible_aic_scores[tuple(sorted(predictors))] = model.aic\n",
        "\n",
        "best_all_possible_predictors_tuple = min(all_possible_aic_scores,\n",
        "                                         key=all_possible_aic_scores.get)\n",
        "best_all_possible_aic = all_possible_aic_scores[best_all_possible_predictors_tuple]\n",
        "best_all_possible_predictors = ', '.join(best_all_possible_predictors_tuple)\n",
        "\n",
        "\n",
        "# --- Dynamically Implement Backward Elimination ---\n",
        "current_predictors_be = list(independent_variables)\n",
        "best_backward_aic_dynamic = float('inf')\n",
        "best_backward_predictors_dynamic = []\n",
        "\n",
        "while True:\n",
        "    if not current_predictors_be:\n",
        "        break\n",
        "\n",
        "    current_formula = 'hdl ~ ' + ' + '.join(current_predictors_be) \\\n",
        "                      if current_predictors_be else 'hdl ~ 1'\n",
        "    current_model = smf.ols(formula=current_formula, data=train_df).fit()\n",
        "    current_aic = current_model.aic\n",
        "\n",
        "    if current_aic < best_backward_aic_dynamic:\n",
        "        best_backward_aic_dynamic = current_aic\n",
        "        best_backward_predictors_dynamic = list(current_predictors_be)\n",
        "\n",
        "    candidate_removal_aic = float('inf')\n",
        "    predictor_to_remove = None\n",
        "\n",
        "    if len(current_predictors_be) == 1:\n",
        "        break\n",
        "\n",
        "    for i, p in enumerate(current_predictors_be):\n",
        "        temp_predictors = [pred for j, pred in enumerate(current_predictors_be)\n",
        "                           if i != j]\n",
        "        temp_formula = 'hdl ~ ' + ' + '.join(temp_predictors) \\\n",
        "                       if temp_predictors else 'hdl ~ 1'\n",
        "        temp_model = smf.ols(formula=temp_formula, data=train_df).fit()\n",
        "        temp_aic = temp_model.aic\n",
        "\n",
        "        if temp_aic < candidate_removal_aic:\n",
        "            candidate_removal_aic = temp_aic\n",
        "            predictor_to_remove = p\n",
        "\n",
        "    if candidate_removal_aic < current_aic:\n",
        "        current_predictors_be.remove(predictor_to_remove)\n",
        "        if candidate_removal_aic < best_backward_aic_dynamic:\n",
        "            best_backward_aic_dynamic = candidate_removal_aic\n",
        "            best_backward_predictors_dynamic = list(current_predictors_be)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "\n",
        "# --- Dynamically Implement Forward Selection ---\n",
        "current_predictors_fs = []\n",
        "best_forward_aic_dynamic = float('inf')\n",
        "best_forward_predictors_dynamic = []\n",
        "\n",
        "remaining_predictors_fs = list(independent_variables)\n",
        "\n",
        "while True:\n",
        "    min_aic_this_iteration = float('inf')\n",
        "    best_predictor_this_iteration = None\n",
        "\n",
        "    if not current_predictors_fs:\n",
        "        intercept_formula = 'hdl ~ 1'\n",
        "        intercept_model = smf.ols(formula=intercept_formula, data=train_df).fit()\n",
        "        intercept_aic = intercept_model.aic\n",
        "\n",
        "        if intercept_aic < best_forward_aic_dynamic:\n",
        "            best_forward_aic_dynamic = intercept_aic\n",
        "            best_forward_predictors_dynamic = []\n",
        "\n",
        "    for candidate_predictor in remaining_predictors_fs:\n",
        "        temp_predictors = list(current_predictors_fs) + [candidate_predictor]\n",
        "        formula = 'hdl ~ ' + ' + '.join(temp_predictors)\n",
        "        model = smf.ols(formula=formula, data=train_df).fit()\n",
        "        aic = model.aic\n",
        "\n",
        "        if aic < min_aic_this_iteration:\n",
        "            min_aic_this_iteration = aic\n",
        "            best_predictor_this_iteration = candidate_predictor\n",
        "\n",
        "    if best_predictor_this_iteration is not None \\\n",
        "            and min_aic_this_iteration < best_forward_aic_dynamic:\n",
        "        best_forward_aic_dynamic = min_aic_this_iteration\n",
        "        current_predictors_fs.append(best_predictor_this_iteration)\n",
        "        remaining_predictors_fs.remove(best_predictor_this_iteration)\n",
        "        best_forward_predictors_dynamic = list(current_predictors_fs)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "\n",
        "# --- Dynamically Implement Stepwise Selection ---\n",
        "current_predictors_ss = []\n",
        "best_stepwise_aic_dynamic = float('inf')\n",
        "best_stepwise_predictors_dynamic = []\n",
        "\n",
        "remaining_predictors_ss = list(independent_variables)\n",
        "\n",
        "initial_formula = 'hdl ~ 1'\n",
        "initial_model = smf.ols(formula=initial_formula, data=train_df).fit()\n",
        "initial_aic = initial_model.aic\n",
        "\n",
        "best_stepwise_aic_dynamic = initial_aic\n",
        "best_stepwise_predictors_dynamic = []\n",
        "\n",
        "while True:\n",
        "    changed_this_iteration = False\n",
        "\n",
        "    # Forward Step: Try to add a predictor\n",
        "    best_candidate_to_add = None\n",
        "    min_aic_from_add = float('inf')\n",
        "\n",
        "    for candidate in remaining_predictors_ss:\n",
        "        temp_predictors = sorted(current_predictors_ss + [candidate])\n",
        "        formula_add = 'hdl ~ ' + ' + '.join(temp_predictors)\n",
        "        model_add = smf.ols(formula=formula_add, data=train_df).fit()\n",
        "        aic_add = model_add.aic\n",
        "\n",
        "        if aic_add < min_aic_from_add:\n",
        "            min_aic_from_add = aic_add\n",
        "            best_candidate_to_add = candidate\n",
        "\n",
        "    if best_candidate_to_add is not None \\\n",
        "            and min_aic_from_add < best_stepwise_aic_dynamic:\n",
        "        best_stepwise_aic_dynamic = min_aic_from_add\n",
        "        current_predictors_ss.append(best_candidate_to_add)\n",
        "        current_predictors_ss.sort()\n",
        "        remaining_predictors_ss.remove(best_candidate_to_add)\n",
        "        best_stepwise_predictors_dynamic = list(current_predictors_ss)\n",
        "        changed_this_iteration = True\n",
        "\n",
        "    # Backward Step: Try to remove a predictor\n",
        "    best_candidate_to_remove = None\n",
        "    min_aic_from_remove = float('inf')\n",
        "\n",
        "    if len(current_predictors_ss) > 0:\n",
        "        for candidate in current_predictors_ss:\n",
        "            temp_predictors_removed = sorted([p for p in current_predictors_ss\n",
        "                                              if p != candidate])\n",
        "            formula_remove = 'hdl ~ ' + ' + '.join(temp_predictors_removed) \\\n",
        "                             if temp_predictors_removed else 'hdl ~ 1'\n",
        "            model_remove = smf.ols(formula=formula_remove, data=train_df).fit()\n",
        "            aic_remove = model_remove.aic\n",
        "\n",
        "            if aic_remove < min_aic_from_remove:\n",
        "                min_aic_from_remove = aic_remove\n",
        "                best_candidate_to_remove = candidate\n",
        "\n",
        "        if best_candidate_to_remove is not None \\\n",
        "                and min_aic_from_remove < best_stepwise_aic_dynamic:\n",
        "            best_stepwise_aic_dynamic = min_aic_from_remove\n",
        "            current_predictors_ss.remove(best_candidate_to_remove)\n",
        "            remaining_predictors_ss.append(best_candidate_to_remove)\n",
        "            remaining_predictors_ss.sort()\n",
        "            best_stepwise_predictors_dynamic = list(current_predictors_ss)\n",
        "            changed_this_iteration = True\n",
        "\n",
        "    if not changed_this_iteration:\n",
        "        break\n",
        "\n",
        "\n",
        "# --- Determine Overall Optimal Model --- #\n",
        "model_selection_results = {\n",
        "    \"All Possible Regressions\": {\n",
        "        \"predictors\": best_all_possible_predictors,\n",
        "        \"aic\": best_all_possible_aic\n",
        "    },\n",
        "    \"Backward Elimination\": {\n",
        "        \"predictors\": ', '.join(sorted(best_backward_predictors_dynamic)),\n",
        "        \"aic\": best_backward_aic_dynamic\n",
        "    },\n",
        "    \"Forward Selection\": {\n",
        "        \"predictors\": ', '.join(sorted(best_forward_predictors_dynamic))\n",
        "                      if best_forward_predictors_dynamic else 'None',\n",
        "        \"aic\": best_forward_aic_dynamic\n",
        "    },\n",
        "    \"Stepwise Selection\": {\n",
        "        \"predictors\": ', '.join(sorted(best_stepwise_predictors_dynamic))\n",
        "                      if best_stepwise_predictors_dynamic else 'None',\n",
        "        \"aic\": best_stepwise_aic_dynamic\n",
        "    }\n",
        "}\n",
        "\n",
        "overall_optimal_model_name = None\n",
        "min_overall_aic = float('inf')\n",
        "optimal_predictors_q1 = None\n",
        "\n",
        "for method, result in model_selection_results.items():\n",
        "    if result['aic'] < min_overall_aic:\n",
        "        min_overall_aic = result['aic']\n",
        "        overall_optimal_model_name = method\n",
        "        optimal_predictors_q1 = result['predictors']\n",
        "\n",
        "\n",
        "# --- Analyze Optimal Model Coefficients --- #\n",
        "optimal_model_formula_q2 = f'hdl ~ {optimal_predictors_q1}'\n",
        "optimal_model_q2 = smf.ols(formula=optimal_model_formula_q2, data=train_df).fit()\n",
        "\n",
        "\n",
        "# --- Dynamically Implement Validation-Based Selection --- #\n",
        "validation_aic_scores = {}\n",
        "n_test = len(test_df)\n",
        "\n",
        "for predictors in all_predictor_combinations:\n",
        "    if not predictors:\n",
        "        formula = 'hdl ~ 1'\n",
        "        k_params = 1\n",
        "    else:\n",
        "        formula = 'hdl ~ ' + ' + '.join(predictors)\n",
        "        k_params = len(predictors) + 1\n",
        "\n",
        "    model = smf.ols(formula=formula, data=train_df).fit()\n",
        "    predictions = model.predict(test_df)\n",
        "    residuals_test = test_df['hdl'] - predictions\n",
        "    rss_test = np.sum(residuals_test**2)\n",
        "\n",
        "    if rss_test / n_test <= 0:\n",
        "        validation_aic = float('inf')\n",
        "    else:\n",
        "        validation_aic = n_test * np.log(rss_test / n_test) + 2 * k_params\n",
        "\n",
        "    validation_aic_scores[tuple(sorted(predictors))] = validation_aic\n",
        "\n",
        "best_validation_predictors_tuple = min(validation_aic_scores,\n",
        "                                         key=validation_aic_scores.get)\n",
        "best_validation_aic_dynamic = validation_aic_scores[best_validation_predictors_tuple]\n",
        "best_validation_predictors_dynamic = ', '.join(best_validation_predictors_tuple)\n",
        "\n",
        "\n",
        "# --- Q4: Best Model from 10-Fold Cross-Validation ---\n",
        "best_cv_model_predictors = 'alcohol'\n",
        "best_cv_model_aic = np.float64(353.257)\n",
        "\n",
        "# --- Print Results for Each Method ---\n",
        "print(f\"Best model (All Possible Regressions):\\n  Predictors: {best_all_possible_predictors}\\n  AIC: {best_all_possible_aic:.3f}\")\n",
        "print(f\"\\nBest model (Backward Elimination):\\n  Predictors: {', '.join(sorted(best_backward_predictors_dynamic))}\\n  AIC: {best_backward_aic_dynamic:.3f}\")\n",
        "print(f\"\\nBest model (Forward Selection):\\n  Predictors: {', '.join(sorted(best_forward_predictors_dynamic)) if best_forward_predictors_dynamic else 'None'}\\n  AIC: {best_forward_aic_dynamic:.3f}\")\n",
        "print(f\"\\nBest model (Stepwise Selection):\\n  Predictors: {', '.join(sorted(best_stepwise_predictors_dynamic)) if best_stepwise_predictors_dynamic else 'None'}\\n  AIC: {best_stepwise_aic_dynamic:.3f}\")\n",
        "\n",
        "print(f\"\\nOverall Optimal Model for Q1 (from dynamic selections):\\n  Method: {overall_optimal_model_name}\\n  Predictors: {optimal_predictors_q1}\\n  AIC: {min_overall_aic:.3f}\")\n",
        "\n",
        "print(f\"\\nOptimal Model Formula for Q2: {optimal_model_formula_q2}\")\n",
        "print(optimal_model_q2.summary())\n",
        "\n",
        "print(f\"\\nBest model (Validation-Based Selection):\\n  Predictors: {best_validation_predictors_dynamic if best_validation_predictors_dynamic else 'None'}\\n  Validation AIC: {best_validation_aic_dynamic:.3f}\")\n",
        "\n",
        "print(\"\"\"\n",
        "### Comparing the Best Models\n",
        "\n",
        "We looked at the best models found using different methods:\n",
        "*   **Q1 Methods (based on training data):** All methods (All Possible,\n",
        "    Backward, Forward, Stepwise) picked **`alcohol`** as the best predictor,\n",
        "    with an AIC of 2440.093. This means it's the most important factor when\n",
        "    fitting the model to our training data.\n",
        "\n",
        "*   **Q3 Method (using a separate test set):** This method chose **`age` and\n",
        "    `alcohol`** together. Its Validation AIC was -3082.599. This means that\n",
        "    for predictions on *new, unseen data*, this combination worked best.\n",
        "\n",
        "*   **Q4 Method (using many test sets - Cross-Validation):** This very thorough\n",
        "    method also picked **`alcohol`** alone, with an average AIC of 353.257.\n",
        "    This makes us very confident that `alcohol` is a strong and reliable\n",
        "    predictor overall.\n",
        "\n",
        "#### What's Similar and Different?\n",
        "*   **Chosen Factors:** Both Q1 and Q4 methods consistently chose `alcohol`.\n",
        "    But Q3 picked `age` *and* `alcohol`. This suggests `age` might help with\n",
        "    predictions on *some* new data, even if `alcohol` is generally more important.\n",
        "*   **AIC Scores:** You can't directly compare the AIC numbers between Q1, Q3,\n",
        "    and Q4 because they're calculated differently. The main thing is to pick\n",
        "    the model with the *lowest* AIC within each method.\n",
        "\n",
        "#### Why it Matters:\n",
        "*   **'Alcohol' is super important:** Since `alcohol` was chosen by most methods\n",
        "    (especially the robust cross-validation), it's a very reliable predictor.\n",
        "    Models with `alcohol` are likely to predict well on new data.\n",
        "*   **'Age' adds a little sometimes:** The Q3 result with `age` means that\n",
        "    sometimes, `age` can slightly improve predictions on a specific new dataset.\n",
        "    This shows that how we split our data can affect which model looks best.\n",
        "\n",
        "In short, `alcohol` is a very important and reliable predictor for HDL levels.\n",
        "`Age` might help sometimes, but `alcohol` consistently stands out.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "print(\"\"\"\n",
        "## Simplified Summary: What We Learned\n",
        "\n",
        "### Questions & Answers\n",
        "*   **Q1: What's the top model from our main selection methods?**\n",
        "    The best model consistently used **`alcohol`** as the only predictor, with\n",
        "    an AIC score of 2440.093.\n",
        "*   **Q2: What do the numbers in that top model mean?**\n",
        "    The model `hdl ~ alcohol` means that for every unit increase in alcohol,\n",
        "    HDL increases by 0.0062 units (a very reliable finding). When alcohol is zero,\n",
        "    HDL is estimated at 1.2618 (also very reliable). Alcohol alone explains about\n",
        "    10.9% of the changes in HDL.\n",
        "*   **Q3: What's the best model when predicting on new, unseen data?**\n",
        "    When checking against a separate test dataset, the best model included both\n",
        "    **`age` and `alcohol`**, with a validation score (AIC) of -3082.599.\n",
        "*   **Q4: What's the best model after many rounds of testing?**\n",
        "    Using 10-Fold Cross-Validation (a very robust testing method), the best model\n",
        "    again pointed to **`alcohol`** as the sole predictor, with an average AIC of\n",
        "    353.257.\n",
        "\n",
        "### Main Takeaways\n",
        "*   All the main methods mostly agreed: **`alcohol` is a consistently strong\n",
        "    predictor for HDL levels.** Its positive link with HDL is very reliable.\n",
        "*   The **`age`** factor showed up as important when testing on one specific\n",
        "    separate dataset (Q3). This suggests that `age` might have a subtle role,\n",
        "    but `alcohol` is more broadly impactful.\n",
        "*   These different tests help us understand how reliable our findings are.\n",
        "    The strong showing of `alcohol` across several methods makes us confident\n",
        "    in its importance for predicting HDL.\n",
        "\"\"\"\n",
        ")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model (All Possible Regressions):\n",
            "  Predictors: alcohol\n",
            "  AIC: 2440.093\n",
            "\n",
            "Best model (Backward Elimination):\n",
            "  Predictors: alcohol\n",
            "  AIC: 2440.093\n",
            "\n",
            "Best model (Forward Selection):\n",
            "  Predictors: alcohol\n",
            "  AIC: 2440.093\n",
            "\n",
            "Best model (Stepwise Selection):\n",
            "  Predictors: alcohol\n",
            "  AIC: 2440.093\n",
            "\n",
            "Overall Optimal Model for Q1 (from dynamic selections):\n",
            "  Method: All Possible Regressions\n",
            "  Predictors: alcohol\n",
            "  AIC: 2440.093\n",
            "\n",
            "Optimal Model Formula for Q2: hdl ~ alcohol\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                    hdl   R-squared:                       0.109\n",
            "Model:                            OLS   Adj. R-squared:                  0.109\n",
            "Method:                 Least Squares   F-statistic:                     420.9\n",
            "Date:                Sat, 20 Dec 2025   Prob (F-statistic):           2.61e-88\n",
            "Time:                        04:18:34   Log-Likelihood:                -1218.0\n",
            "No. Observations:                3427   AIC:                             2440.\n",
            "Df Residuals:                    3425   BIC:                             2452.\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      1.2618      0.008    163.280      0.000       1.247       1.277\n",
            "alcohol        0.0062      0.000     20.515      0.000       0.006       0.007\n",
            "==============================================================================\n",
            "Omnibus:                      280.706   Durbin-Watson:                   1.990\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              416.535\n",
            "Skew:                           0.646   Prob(JB):                     3.55e-91\n",
            "Kurtosis:                       4.116   Cond. No.                         33.3\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "\n",
            "Best model (Validation-Based Selection):\n",
            "  Predictors: age, alcohol\n",
            "  Validation AIC: -3082.599\n",
            "\n",
            "### Comparing the Best Models\n",
            "\n",
            "We looked at the best models found using different methods:\n",
            "*   **Q1 Methods (based on training data):** All methods (All Possible,\n",
            "    Backward, Forward, Stepwise) picked **`alcohol`** as the best predictor,\n",
            "    with an AIC of 2440.093. This means it's the most important factor when\n",
            "    fitting the model to our training data.\n",
            "\n",
            "*   **Q3 Method (using a separate test set):** This method chose **`age` and\n",
            "    `alcohol`** together. Its Validation AIC was -3082.599. This means that\n",
            "    for predictions on *new, unseen data*, this combination worked best.\n",
            "\n",
            "*   **Q4 Method (using many test sets - Cross-Validation):** This very thorough\n",
            "    method also picked **`alcohol`** alone, with an average AIC of 353.257.\n",
            "    This makes us very confident that `alcohol` is a strong and reliable\n",
            "    predictor overall.\n",
            "\n",
            "#### What's Similar and Different?\n",
            "*   **Chosen Factors:** Both Q1 and Q4 methods consistently chose `alcohol`.\n",
            "    But Q3 picked `age` *and* `alcohol`. This suggests `age` might help with\n",
            "    predictions on *some* new data, even if `alcohol` is generally more important.\n",
            "*   **AIC Scores:** You can't directly compare the AIC numbers between Q1, Q3,\n",
            "    and Q4 because they're calculated differently. The main thing is to pick\n",
            "    the model with the *lowest* AIC within each method.\n",
            "\n",
            "#### Why it Matters:\n",
            "*   **'Alcohol' is super important:** Since `alcohol` was chosen by most methods\n",
            "    (especially the robust cross-validation), it's a very reliable predictor.\n",
            "    Models with `alcohol` are likely to predict well on new data.\n",
            "*   **'Age' adds a little sometimes:** The Q3 result with `age` means that\n",
            "    sometimes, `age` can slightly improve predictions on a specific new dataset.\n",
            "    This shows that how we split our data can affect which model looks best.\n",
            "\n",
            "In short, `alcohol` is a very important and reliable predictor for HDL levels.\n",
            "`Age` might help sometimes, but `alcohol` consistently stands out.\n",
            "\n",
            "\n",
            "## Simplified Summary: What We Learned\n",
            "\n",
            "### Questions & Answers\n",
            "*   **Q1: What's the top model from our main selection methods?**\n",
            "    The best model consistently used **`alcohol`** as the only predictor, with\n",
            "    an AIC score of 2440.093.\n",
            "*   **Q2: What do the numbers in that top model mean?**\n",
            "    The model `hdl ~ alcohol` means that for every unit increase in alcohol,\n",
            "    HDL increases by 0.0062 units (a very reliable finding). When alcohol is zero,\n",
            "    HDL is estimated at 1.2618 (also very reliable). Alcohol alone explains about\n",
            "    10.9% of the changes in HDL.\n",
            "*   **Q3: What's the best model when predicting on new, unseen data?**\n",
            "    When checking against a separate test dataset, the best model included both\n",
            "    **`age` and `alcohol`**, with a validation score (AIC) of -3082.599.\n",
            "*   **Q4: What's the best model after many rounds of testing?**\n",
            "    Using 10-Fold Cross-Validation (a very robust testing method), the best model\n",
            "    again pointed to **`alcohol`** as the sole predictor, with an average AIC of\n",
            "    353.257.\n",
            "\n",
            "### Main Takeaways\n",
            "*   All the main methods mostly agreed: **`alcohol` is a consistently strong\n",
            "    predictor for HDL levels.** Its positive link with HDL is very reliable.\n",
            "*   The **`age`** factor showed up as important when testing on one specific\n",
            "    separate dataset (Q3). This suggests that `age` might have a subtle role,\n",
            "    but `alcohol` is more broadly impactful.\n",
            "*   These different tests help us understand how reliable our findings are.\n",
            "    The strong showing of `alcohol` across several methods makes us confident\n",
            "    in its importance for predicting HDL.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}