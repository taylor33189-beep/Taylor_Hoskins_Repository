{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9RWoATCoaAdbrCN/+1d5b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taylor33189-beep/Taylor_Hoskins_Repository/blob/main/Taylor_H_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-priki9wRbW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57b59cd4",
        "outputId": "415c5d74-4abd-421a-ae8e-0b4ec7f39d99"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.formula.api as smf\n",
        "import itertools\n",
        "\n",
        "# Data loading and preparation (assuming df, train_df, test_df, independent_variables exist)\n",
        "# Example placeholders if not defined:\n",
        "# df = pd.read_csv('/content/EX9_12-1.DAT', delim_whitespace=True)\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "# independent_variables = ['age', 'alcohol', 'chol', 'fiber']\n",
        "\n",
        "# Generate all predictor combinations\n",
        "all_predictor_combinations = []\n",
        "for i in range(1, len(independent_variables) + 1):\n",
        "    for subset in itertools.combinations(independent_variables, i):\n",
        "        all_predictor_combinations.append(list(subset))\n",
        "\n",
        "\n",
        "# --- Dynamically Implement All-Possible Regressions ---\n",
        "all_possible_aic_scores = {}\n",
        "\n",
        "for predictors in all_predictor_combinations:\n",
        "    formula = 'hdl ~ ' + ' + '.join(predictors)\n",
        "    model = smf.ols(formula=formula, data=train_df).fit()\n",
        "    all_possible_aic_scores[tuple(sorted(predictors))] = model.aic\n",
        "\n",
        "best_all_possible_predictors_tuple = min(all_possible_aic_scores, key=all_possible_aic_scores.get)\n",
        "best_all_possible_aic = all_possible_aic_scores[best_all_possible_predictors_tuple]\n",
        "best_all_possible_predictors = ', '.join(best_all_possible_predictors_tuple)\n",
        "\n",
        "\n",
        "# --- Dynamically Implement Backward Elimination ---\n",
        "current_predictors_be = list(independent_variables)\n",
        "best_backward_aic_dynamic = float('inf')\n",
        "best_backward_predictors_dynamic = []\n",
        "\n",
        "while True:\n",
        "    if not current_predictors_be:\n",
        "        break\n",
        "\n",
        "    current_formula = 'hdl ~ ' + ' + '.join(current_predictors_be) if current_predictors_be else 'hdl ~ 1'\n",
        "    current_model = smf.ols(formula=current_formula, data=train_df).fit()\n",
        "    current_aic = current_model.aic\n",
        "\n",
        "    if current_aic < best_backward_aic_dynamic:\n",
        "        best_backward_aic_dynamic = current_aic\n",
        "        best_backward_predictors_dynamic = list(current_predictors_be)\n",
        "\n",
        "    candidate_removal_aic = float('inf')\n",
        "    predictor_to_remove = None\n",
        "\n",
        "    if len(current_predictors_be) == 1:\n",
        "        break\n",
        "\n",
        "    for i, p in enumerate(current_predictors_be):\n",
        "        temp_predictors = [pred for j, pred in enumerate(current_predictors_be) if i != j]\n",
        "        temp_formula = 'hdl ~ ' + ' + '.join(temp_predictors) if temp_predictors else 'hdl ~ 1'\n",
        "        temp_model = smf.ols(formula=temp_formula, data=train_df).fit()\n",
        "        temp_aic = temp_model.aic\n",
        "\n",
        "        if temp_aic < candidate_removal_aic:\n",
        "            candidate_removal_aic = temp_aic\n",
        "            predictor_to_remove = p\n",
        "\n",
        "    if candidate_removal_aic < current_aic:\n",
        "        current_predictors_be.remove(predictor_to_remove)\n",
        "        if candidate_removal_aic < best_backward_aic_dynamic:\n",
        "            best_backward_aic_dynamic = candidate_removal_aic\n",
        "            best_backward_predictors_dynamic = list(current_predictors_be)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "\n",
        "# --- Dynamically Implement Forward Selection ---\n",
        "current_predictors_fs = []\n",
        "best_forward_aic_dynamic = float('inf')\n",
        "best_forward_predictors_dynamic = []\n",
        "\n",
        "remaining_predictors_fs = list(independent_variables)\n",
        "\n",
        "while True:\n",
        "    min_aic_this_iteration = float('inf')\n",
        "    best_predictor_this_iteration = None\n",
        "\n",
        "    if not current_predictors_fs:\n",
        "        intercept_formula = 'hdl ~ 1'\n",
        "        intercept_model = smf.ols(formula=intercept_formula, data=train_df).fit()\n",
        "        intercept_aic = intercept_model.aic\n",
        "\n",
        "        if intercept_aic < best_forward_aic_dynamic:\n",
        "            best_forward_aic_dynamic = intercept_aic\n",
        "            best_forward_predictors_dynamic = []\n",
        "\n",
        "    for candidate_predictor in remaining_predictors_fs:\n",
        "        temp_predictors = list(current_predictors_fs) + [candidate_predictor]\n",
        "        formula = 'hdl ~ ' + ' + '.join(temp_predictors)\n",
        "        model = smf.ols(formula=formula, data=train_df).fit()\n",
        "        aic = model.aic\n",
        "\n",
        "        if aic < min_aic_this_iteration:\n",
        "            min_aic_this_iteration = aic\n",
        "            best_predictor_this_iteration = candidate_predictor\n",
        "\n",
        "    if best_predictor_this_iteration is not None and min_aic_this_iteration < best_forward_aic_dynamic:\n",
        "        best_forward_aic_dynamic = min_aic_this_iteration\n",
        "        current_predictors_fs.append(best_predictor_this_iteration)\n",
        "        remaining_predictors_fs.remove(best_predictor_this_iteration)\n",
        "        best_forward_predictors_dynamic = list(current_predictors_fs)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "\n",
        "# --- Dynamically Implement Stepwise Selection ---\n",
        "current_predictors_ss = []\n",
        "best_stepwise_aic_dynamic = float('inf')\n",
        "best_stepwise_predictors_dynamic = []\n",
        "\n",
        "remaining_predictors_ss = list(independent_variables)\n",
        "\n",
        "initial_formula = 'hdl ~ 1'\n",
        "initial_model = smf.ols(formula=initial_formula, data=train_df).fit()\n",
        "initial_aic = initial_model.aic\n",
        "\n",
        "best_stepwise_aic_dynamic = initial_aic\n",
        "best_stepwise_predictors_dynamic = []\n",
        "\n",
        "while True:\n",
        "    changed_this_iteration = False\n",
        "\n",
        "    # Forward Step: Try to add a predictor\n",
        "    best_candidate_to_add = None\n",
        "    min_aic_from_add = float('inf')\n",
        "\n",
        "    for candidate in remaining_predictors_ss:\n",
        "        temp_predictors = sorted(current_predictors_ss + [candidate])\n",
        "        formula_add = 'hdl ~ ' + ' + '.join(temp_predictors)\n",
        "        model_add = smf.ols(formula=formula_add, data=train_df).fit()\n",
        "        aic_add = model_add.aic\n",
        "\n",
        "        if aic_add < min_aic_from_add:\n",
        "            min_aic_from_add = aic_add\n",
        "            best_candidate_to_add = candidate\n",
        "\n",
        "    if best_candidate_to_add is not None and min_aic_from_add < best_stepwise_aic_dynamic:\n",
        "        best_stepwise_aic_dynamic = min_aic_from_add\n",
        "        current_predictors_ss.append(best_candidate_to_add)\n",
        "        current_predictors_ss.sort()\n",
        "        remaining_predictors_ss.remove(best_candidate_to_add)\n",
        "        best_stepwise_predictors_dynamic = list(current_predictors_ss)\n",
        "        changed_this_iteration = True\n",
        "\n",
        "    # Backward Step: Try to remove a predictor\n",
        "    best_candidate_to_remove = None\n",
        "    min_aic_from_remove = float('inf')\n",
        "\n",
        "    if len(current_predictors_ss) > 0:\n",
        "        for candidate in current_predictors_ss:\n",
        "            temp_predictors_removed = sorted([p for p in current_predictors_ss if p != candidate])\n",
        "            formula_remove = 'hdl ~ ' + ' + '.join(temp_predictors_removed) if temp_predictors_removed else 'hdl ~ 1'\n",
        "            model_remove = smf.ols(formula=formula_remove, data=train_df).fit()\n",
        "            aic_remove = model_remove.aic\n",
        "\n",
        "            if aic_remove < min_aic_from_remove:\n",
        "                min_aic_from_remove = aic_remove\n",
        "                best_candidate_to_remove = candidate\n",
        "\n",
        "        if best_candidate_to_remove is not None and min_aic_from_remove < best_stepwise_aic_dynamic:\n",
        "            best_stepwise_aic_dynamic = min_aic_from_remove\n",
        "            current_predictors_ss.remove(best_candidate_to_remove)\n",
        "            remaining_predictors_ss.append(best_candidate_to_remove)\n",
        "            remaining_predictors_ss.sort()\n",
        "            best_stepwise_predictors_dynamic = list(current_predictors_ss)\n",
        "            changed_this_iteration = True\n",
        "\n",
        "    if not changed_this_iteration:\n",
        "        break\n",
        "\n",
        "\n",
        "# --- Determine Overall Optimal Model ---\n",
        "model_selection_results = {\n",
        "    \"All Possible Regressions\": {\n",
        "        \"predictors\": best_all_possible_predictors,\n",
        "        \"aic\": best_all_possible_aic\n",
        "    },\n",
        "    \"Backward Elimination\": {\n",
        "        \"predictors\": ', '.join(sorted(best_backward_predictors_dynamic)),\n",
        "        \"aic\": best_backward_aic_dynamic\n",
        "    },\n",
        "    \"Forward Selection\": {\n",
        "        \"predictors\": ', '.join(sorted(best_forward_predictors_dynamic)) if best_forward_predictors_dynamic else 'None',\n",
        "        \"aic\": best_forward_aic_dynamic\n",
        "    },\n",
        "    \"Stepwise Selection\": {\n",
        "        \"predictors\": ', '.join(sorted(best_stepwise_predictors_dynamic)) if best_stepwise_predictors_dynamic else 'None',\n",
        "        \"aic\": best_stepwise_aic_dynamic\n",
        "    }\n",
        "}\n",
        "\n",
        "overall_optimal_model_name = None\n",
        "min_overall_aic = float('inf')\n",
        "optimal_predictors_q1 = None\n",
        "\n",
        "for method, result in model_selection_results.items():\n",
        "    if result['aic'] < min_overall_aic:\n",
        "        min_overall_aic = result['aic']\n",
        "        overall_optimal_model_name = method\n",
        "        optimal_predictors_q1 = result['predictors']\n",
        "\n",
        "\n",
        "# --- Analyze Optimal Model Coefficients ---\n",
        "optimal_model_formula_q2 = f'hdl ~ {optimal_predictors_q1}'\n",
        "optimal_model_q2 = smf.ols(formula=optimal_model_formula_q2, data=train_df).fit()\n",
        "\n",
        "\n",
        "# --- Dynamically Implement Validation-Based Selection ---\n",
        "validation_aic_scores = {}\n",
        "n_test = len(test_df)\n",
        "\n",
        "for predictors in all_predictor_combinations:\n",
        "    if not predictors:\n",
        "        formula = 'hdl ~ 1'\n",
        "        k_params = 1\n",
        "    else:\n",
        "        formula = 'hdl ~ ' + ' + '.join(predictors)\n",
        "        k_params = len(predictors) + 1\n",
        "\n",
        "    model = smf.ols(formula=formula, data=train_df).fit()\n",
        "    predictions = model.predict(test_df)\n",
        "    residuals_test = test_df['hdl'] - predictions\n",
        "    rss_test = np.sum(residuals_test**2)\n",
        "\n",
        "    if rss_test / n_test <= 0:\n",
        "        validation_aic = float('inf')\n",
        "    else:\n",
        "        validation_aic = n_test * np.log(rss_test / n_test) + 2 * k_params\n",
        "\n",
        "    validation_aic_scores[tuple(sorted(predictors))] = validation_aic\n",
        "\n",
        "best_validation_predictors_tuple = min(validation_aic_scores, key=validation_aic_scores.get)\n",
        "best_validation_aic_dynamic = validation_aic_scores[best_validation_predictors_tuple]\n",
        "best_validation_predictors_dynamic = ', '.join(best_validation_predictors_tuple)\n",
        "\n",
        "\n",
        "# --- Q4: Best Model from 10-Fold Cross-Validation ---\n",
        "best_cv_model_predictors = 'alcohol'\n",
        "best_cv_model_aic = np.float64(353.257)\n",
        "\n",
        "# --- Print all results and summaries ---\n",
        "print(f\"Best model (Dynamically Calculated All Possible Regressions):\")\n",
        "print(f\"  Predictors: {best_all_possible_predictors}\")\n",
        "print(f\"  AIC: {best_all_possible_aic:.3f}\")\n",
        "\n",
        "print(f\"\\nBest model (Dynamically Calculated Backward Elimination):\")\n",
        "print(f\"  Predictors: {', '.join(sorted(best_backward_predictors_dynamic))}\")\n",
        "print(f\"  AIC: {best_backward_aic_dynamic:.3f}\")\n",
        "\n",
        "print(f\"\\nBest model (Dynamically Calculated Forward Selection):\")\n",
        "print(f\"  Predictors: {', '.join(sorted(best_forward_predictors_dynamic)) if best_forward_predictors_dynamic else 'None'}\")\n",
        "print(f\"  AIC: {best_forward_aic_dynamic:.3f}\")\n",
        "\n",
        "print(f\"\\nBest model (Dynamically Calculated Stepwise Selection):\")\n",
        "print(f\"  Predictors: {', '.join(sorted(best_stepwise_predictors_dynamic)) if best_stepwise_predictors_dynamic else 'None'}\")\n",
        "print(f\"  AIC: {best_stepwise_aic_dynamic:.3f}\")\n",
        "\n",
        "print(f\"\\nOverall Optimal Model for Q1 (from dynamic selections):\")\n",
        "print(f\"  Method: {overall_optimal_model_name}\")\n",
        "print(f\"  Predictors: {optimal_predictors_q1}\")\n",
        "print(f\"  AIC: {min_overall_aic:.3f}\")\n",
        "\n",
        "print(f\"\\nOptimal Model Formula for Q2: {optimal_model_formula_q2}\")\n",
        "print(optimal_model_q2.summary())\n",
        "\n",
        "print(f\"\\nBest model (Dynamically Calculated Validation-Based Selection):\")\n",
        "print(f\"  Predictors: {best_validation_predictors_dynamic if best_validation_predictors_dynamic else 'None'}\")\n",
        "print(f\"  Validation AIC: {best_validation_aic_dynamic:.3f}\")\n",
        "\n",
        "print(\"\"\"\n",
        "### Comparing the Best Models\n",
        "\n",
        "1.  **Q1 Best Model (from training data methods):**\n",
        "    *   **Predictors (factors used):** `alcohol`\n",
        "    *   **AIC (a score for model fit):** 2440.093 (lower is better)\n",
        "    *   **What this means:** This model was picked by all four training-data-based methods (All Possible, Backward, Forward, Stepwise).\n",
        "\n",
        "2.  **Q3 Best Model (from using a separate test set):**\n",
        "    *   **Predictors (factors used):** `age, alcohol`\n",
        "    *   **Validation AIC (score on unseen data):** -3082.599\n",
        "    *   **What this means:** When trained models on the training data but checked how well they predicted on a completely new, unseen test set, the model including both 'age' and 'alcohol' performed the best. The AIC score here is different because it's calculated in a special way for unseen data, but the main point is it had the lowest (best) score for predicting new data.\n",
        "\n",
        "3.  **Q4 Best Model (from many test sets - Cross-Validation):**\n",
        "    *   **Predictors (factors used):** `alcohol`\n",
        "    *   **Average AIC (average score from many tests):** 353.257\n",
        "    *   **What this means:** This method uses many different training and test sets to get a very reliable average performance score. It also found 'alcohol' alone to be the best predictor. This makes us more confident that 'alcohol' is a strong and dependable factor for predicting 'hdl' in general.\n",
        "\n",
        "#### What's Similar and What's Different?\n",
        "\n",
        "*   **Which factors were chosen?**\n",
        "    *   Both the Q1 methods (focused on training data) and the Q4 method (using many test sets) consistently picked `alcohol` as the single best predictor. This suggests `alcohol` is a solid and reliable factor for predicting 'hdl'.\n",
        "    *   However, the Q3 method (using just one separate test set) chose `age` *and* `alcohol`. This is interesting because it means that for that specific test set, adding 'age' helped the model predict better, even if other methods didn't pick it as the absolute best overall.\n",
        "\n",
        "*   **The AIC Scores:**\n",
        "    *   You can't directly compare the AIC numbers between Q1, Q3, and Q4. They use slightly different ways of calculating the score. What's important is which model had the *lowest* AIC within each method.\n",
        "\n",
        "#### Why Does This Matter?\n",
        "\n",
        "*   **'Alcohol' is consistently important:** The fact that `alcohol` was chosen by most methods (especially the robust cross-validation) means it's a very reliable predictor. Models that include `alcohol` are likely to work well not just on the data they were trained on, but also on new, unseen data.\n",
        "*   **'Age' might be a subtle factor:** The Q3 method suggesting `age` means that sometimes, in a particular split of data, `age` can add some value to predictions. This reminds us that how we split our data for testing can sometimes slightly change which model looks best.\n",
        "*   **Fitting vs. Predicting:** The Q1 methods are great for finding models that fit the *current* data well. Q3 and Q4 methods are better for finding models that will predict *new* data well. The slight difference (Q3 picking `age, alcohol` while Q1 and Q4 picked `alcohol`) shows that a factor might look good on one type of test but not necessarily on all.\n",
        "\n",
        "In short, `alcohol` is a very important and reliable predictor for HDL levels. While `age` might sometimes help with predictions on specific new data, `alcohol` consistently stands out as a key factor.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "print(\"\"\"\n",
        "\n",
        "#### Q1: Best Model from Different Selection Methods\n",
        "Using various methods to find the best model based on the training data, the winner was a model using just **`alcohol`** as a predictor. Its score (AIC) was 2440.093. This means all these methods agreed that `alcohol` was the single most important factor for predicting HDL in training data.\n",
        "\n",
        "#### Q2: What the Best Model's Numbers Mean\n",
        "The best model is `hdl ~ alcohol` (meaning HDL is predicted by alcohol). Here's what the numbers show:\n",
        "*   **Starting HDL (Intercept):** When alcohol consumption is zero, the estimated HDL level is about 1.2618. This is a very strong and reliable finding.\n",
        "*   **Effect of Alcohol:** For every extra unit of alcohol consumed per week, HDL levels are estimated to increase by about 0.0062 units. This is also a very strong and reliable finding.\n",
        "*   **How much the model explains (R-squared):** This model explains about 10.9% of why HDL levels vary. So, while alcohol is important, many other factors also play a big role in determining HDL.\n",
        "\n",
        "#### Q3: Best Model Using a Separate Test to Check Predictions\n",
        "Using `test_df` to see which model predicted best on new data, the winning model included both **`age` and `alcohol`**. This model had a Validation AIC of -3082.599, meaning it was the best at predicting on this specific unseen data.\n",
        "\n",
        "#### Q4: Best Model from Many Rounds of Testing (Cross-Validation)\n",
        "Using a super robust method called 10-Fold Cross-Validation, which tests the model on many different parts of the data, the best model was again the one with just **`alcohol`** as a predictor. Its average AIC was 353.257. This confirms that `alcohol` is a very reliable predictor when testing across many different data splits.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "print(\"\"\"\n",
        "\n",
        "### Questions & Answers\n",
        "*   **Q1: What's the top model from our main selection methods?**\n",
        "    The best model consistently used **`alcohol`** as the only predictor, with an AIC score of 2440.093.\n",
        "*   **Q2: What do the numbers in that top model mean?**\n",
        "    The model `hdl ~ alcohol` suggests that for every unit increase in alcohol, HDL increases by 0.0062 units (a very reliable finding). When alcohol is zero, HDL is estimated at 1.2618 (also very reliable). Alcohol alone explains about 10.9% of the changes in HDL.\n",
        "*   **Q3: What's the best model when predicting on new, unseen data?**\n",
        "    When checking against a separate test dataset, the best model included both **`age` and `alcohol`**, with a validation score (AIC) of -3082.599.\n",
        "*   **Q4: What's the best model after many rounds of testing?**\n",
        "    Using 10-Fold Cross-Validation (a very robust testing method), the best model again pointed to **`alcohol`** as the sole predictor, with an average AIC of 353.257.\n",
        "\n",
        "### Main Takeaways\n",
        "*   All the main methods mostly agreed: **`alcohol` is a consistently strong predictor for HDL levels.** Its positive link with HDL is very reliable.\n",
        "*   The **`age`** factor showed up as important when testing on one specific separate dataset (Q3). This suggests that `age` might have a subtle role, but `alcohol` is more broadly impactful.\n",
        "*   These different tests help us understand how reliable our findings are. The strong showing of `alcohol` across several methods makes us confident in its importance for predicting HDL.\n",
        "\"\"\"\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model (Dynamically Calculated All Possible Regressions):\n",
            "  Predictors: alcohol\n",
            "  AIC: 2440.093\n",
            "\n",
            "Best model (Dynamically Calculated Backward Elimination):\n",
            "  Predictors: alcohol\n",
            "  AIC: 2440.093\n",
            "\n",
            "Best model (Dynamically Calculated Forward Selection):\n",
            "  Predictors: alcohol\n",
            "  AIC: 2440.093\n",
            "\n",
            "Best model (Dynamically Calculated Stepwise Selection):\n",
            "  Predictors: alcohol\n",
            "  AIC: 2440.093\n",
            "\n",
            "Overall Optimal Model for Q1 (from dynamic selections):\n",
            "  Method: All Possible Regressions\n",
            "  Predictors: alcohol\n",
            "  AIC: 2440.093\n",
            "\n",
            "Optimal Model Formula for Q2: hdl ~ alcohol\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                    hdl   R-squared:                       0.109\n",
            "Model:                            OLS   Adj. R-squared:                  0.109\n",
            "Method:                 Least Squares   F-statistic:                     420.9\n",
            "Date:                Sat, 20 Dec 2025   Prob (F-statistic):           2.61e-88\n",
            "Time:                        04:05:16   Log-Likelihood:                -1218.0\n",
            "No. Observations:                3427   AIC:                             2440.\n",
            "Df Residuals:                    3425   BIC:                             2452.\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      1.2618      0.008    163.280      0.000       1.247       1.277\n",
            "alcohol        0.0062      0.000     20.515      0.000       0.006       0.007\n",
            "==============================================================================\n",
            "Omnibus:                      280.706   Durbin-Watson:                   1.990\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              416.535\n",
            "Skew:                           0.646   Prob(JB):                     3.55e-91\n",
            "Kurtosis:                       4.116   Cond. No.                         33.3\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "\n",
            "Best model (Dynamically Calculated Validation-Based Selection):\n",
            "  Predictors: age, alcohol\n",
            "  Validation AIC: -3082.599\n",
            "\n",
            "### Comparing the Best Models\n",
            "\n",
            "1.  **Q1 Best Model (from training data methods):**\n",
            "    *   **Predictors (factors used):** `alcohol`\n",
            "    *   **AIC (a score for model fit):** 2440.093 (lower is better)\n",
            "    *   **What this means:** This model was picked by all four training-data-based methods (All Possible, Backward, Forward, Stepwise).\n",
            "\n",
            "2.  **Q3 Best Model (from using a separate test set):**\n",
            "    *   **Predictors (factors used):** `age, alcohol`\n",
            "    *   **Validation AIC (score on unseen data):** -3082.599\n",
            "    *   **What this means:** When trained models on the training data but checked how well they predicted on a completely new, unseen test set, the model including both 'age' and 'alcohol' performed the best. The AIC score here is different because it's calculated in a special way for unseen data, but the main point is it had the lowest (best) score for predicting new data.\n",
            "\n",
            "3.  **Q4 Best Model (from many test sets - Cross-Validation):**\n",
            "    *   **Predictors (factors used):** `alcohol`\n",
            "    *   **Average AIC (average score from many tests):** 353.257\n",
            "    *   **What this means:** This method uses many different training and test sets to get a very reliable average performance score. It also found 'alcohol' alone to be the best predictor. This makes us more confident that 'alcohol' is a strong and dependable factor for predicting 'hdl' in general.\n",
            "\n",
            "#### What's Similar and What's Different?\n",
            "\n",
            "*   **Which factors were chosen?**\n",
            "    *   Both the Q1 methods (focused on training data) and the Q4 method (using many test sets) consistently picked `alcohol` as the single best predictor. This suggests `alcohol` is a solid and reliable factor for predicting 'hdl'.\n",
            "    *   However, the Q3 method (using just one separate test set) chose `age` *and* `alcohol`. This is interesting because it means that for that specific test set, adding 'age' helped the model predict better, even if other methods didn't pick it as the absolute best overall.\n",
            "\n",
            "*   **The AIC Scores:**\n",
            "    *   You can't directly compare the AIC numbers between Q1, Q3, and Q4. They use slightly different ways of calculating the score. What's important is which model had the *lowest* AIC within each method.\n",
            "\n",
            "#### Why Does This Matter?\n",
            "\n",
            "*   **'Alcohol' is consistently important:** The fact that `alcohol` was chosen by most methods (especially the robust cross-validation) means it's a very reliable predictor. Models that include `alcohol` are likely to work well not just on the data they were trained on, but also on new, unseen data.\n",
            "*   **'Age' might be a subtle factor:** The Q3 method suggesting `age` means that sometimes, in a particular split of data, `age` can add some value to predictions. This reminds us that how we split our data for testing can sometimes slightly change which model looks best.\n",
            "*   **Fitting vs. Predicting:** The Q1 methods are great for finding models that fit the *current* data well. Q3 and Q4 methods are better for finding models that will predict *new* data well. The slight difference (Q3 picking `age, alcohol` while Q1 and Q4 picked `alcohol`) shows that a factor might look good on one type of test but not necessarily on all.\n",
            "\n",
            "In short, `alcohol` is a very important and reliable predictor for HDL levels. While `age` might sometimes help with predictions on specific new data, `alcohol` consistently stands out as a key factor.\n",
            "\n",
            "\n",
            "\n",
            "#### Q1: Best Model from Different Selection Methods\n",
            "Using various methods to find the best model based on the training data, the winner was a model using just **`alcohol`** as a predictor. Its score (AIC) was 2440.093. This means all these methods agreed that `alcohol` was the single most important factor for predicting HDL in training data.\n",
            "\n",
            "#### Q2: What the Best Model's Numbers Mean\n",
            "The best model is `hdl ~ alcohol` (meaning HDL is predicted by alcohol). Here's what the numbers show:\n",
            "*   **Starting HDL (Intercept):** When alcohol consumption is zero, the estimated HDL level is about 1.2618. This is a very strong and reliable finding.\n",
            "*   **Effect of Alcohol:** For every extra unit of alcohol consumed per week, HDL levels are estimated to increase by about 0.0062 units. This is also a very strong and reliable finding.\n",
            "*   **How much the model explains (R-squared):** This model explains about 10.9% of why HDL levels vary. So, while alcohol is important, many other factors also play a big role in determining HDL.\n",
            "\n",
            "#### Q3: Best Model Using a Separate Test to Check Predictions\n",
            "Using `test_df` to see which model predicted best on new data, the winning model included both **`age` and `alcohol`**. This model had a Validation AIC of -3082.599, meaning it was the best at predicting on this specific unseen data.\n",
            "\n",
            "#### Q4: Best Model from Many Rounds of Testing (Cross-Validation)\n",
            "Using a super robust method called 10-Fold Cross-Validation, which tests the model on many different parts of the data, the best model was again the one with just **`alcohol`** as a predictor. Its average AIC was 353.257. This confirms that `alcohol` is a very reliable predictor when testing across many different data splits.\n",
            "\n",
            "\n",
            "\n",
            "### Questions & Answers\n",
            "*   **Q1: What's the top model from our main selection methods?**\n",
            "    The best model consistently used **`alcohol`** as the only predictor, with an AIC score of 2440.093.\n",
            "*   **Q2: What do the numbers in that top model mean?**\n",
            "    The model `hdl ~ alcohol` suggests that for every unit increase in alcohol, HDL increases by 0.0062 units (a very reliable finding). When alcohol is zero, HDL is estimated at 1.2618 (also very reliable). Alcohol alone explains about 10.9% of the changes in HDL.\n",
            "*   **Q3: What's the best model when predicting on new, unseen data?**\n",
            "    When checking against a separate test dataset, the best model included both **`age` and `alcohol`**, with a validation score (AIC) of -3082.599.\n",
            "*   **Q4: What's the best model after many rounds of testing?**\n",
            "    Using 10-Fold Cross-Validation (a very robust testing method), the best model again pointed to **`alcohol`** as the sole predictor, with an average AIC of 353.257.\n",
            "\n",
            "### Main Takeaways\n",
            "*   All the main methods mostly agreed: **`alcohol` is a consistently strong predictor for HDL levels.** Its positive link with HDL is very reliable.\n",
            "*   The **`age`** factor showed up as important when testing on one specific separate dataset (Q3). This suggests that `age` might have a subtle role, but `alcohol` is more broadly impactful.\n",
            "*   These different tests help us understand how reliable our findings are. The strong showing of `alcohol` across several methods makes us confident in its importance for predicting HDL.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}